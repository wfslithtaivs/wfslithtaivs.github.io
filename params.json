{"name":"Wfslithtaivs.GitHub.io","tagline":"Practical Machine Learning, Project July 2015","body":"<h1 class=\"title\">Practical Machine Learning, Project July ‘15</h1>\r\n<h4 class=\"author\"><em>Kate Sergeeva</em></h4>\r\n<div id=\"project-summary\" class=\"section level2\">\r\n<h2>Project summary</h2>\r\n<p>The goal of this project is using machine learning techniques to find a best model which will be able to predict how good athletes are doing the particular types of exercises and helps to catch the common mistakes.</p>\r\n<p>Data for this project kindly provided by a group of enthusiasts who take measurements about themselves regularly using devices such as Jawbone Up, Nike FuelBand, and Fitbit to find patterns in their behavior to improve their health.</p>\r\n<p>We will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.</p>\r\n<p><em>More information is available from the website here: <a href=\"http://groupware.les.inf.puc-rio.br/har\" class=\"uri\">http://groupware.les.inf.puc-rio.br/har</a> (see the section on the Weight Lifting Exercise Dataset).</em></p>\r\n<p>Project report will describe the following: \r\n<ul>\r\n<li>how the model was built,</li>\r\n<li>how the cross-validation was used,</li>\r\n<li>how out of sample error was estimated,</li>\r\n<li>how other choices been made.</li>\r\n</ul>\r\n</p>\r\n<p>And provide prediction for 20 different test cases.</p>\r\n</div>\r\n<div id=\"contents\" class=\"section level2\">\r\n<h2>Contents</h2>\r\n<ul>\r\n<li>Loading and cleansing data</li>\r\n<li>Building models</li>\r\n<li>Cross-validation to measure “out of sample error”</li>\r\n<li>Model selection</li>\r\n<li>Report the results of test cases processing</li>\r\n</ul>\r\n</div>\r\n<div id=\"loading-and-cleansing-data\" class=\"section level2\">\r\n<h2>Loading and cleansing data</h2>\r\n<pre class=\"r\"><code>train &lt;- read.csv(file = &quot;/Users/macbookpro/Downloads/pml-training.csv&quot;, header = TRUE, sep = &quot;,&quot;)\r\ntest &lt;- read.csv(file = &quot;/Users/macbookpro/Downloads/pml-testing.csv&quot;, header = TRUE, sep = &quot;,&quot;)</code></pre>\r\n<div id=\"removing-meaningless-variables\" class=\"section level3\">\r\n<h3>Removing meaningless variables</h3>\r\n<p>The test cases require predictions for particular moments with no connections to the style of particular athlet and it allows us to simply exclude from the following consideration the name of athlet and any time series data and row numbers.</p>\r\n<pre class=\"r\"><code>modelTrain &lt;- subset(train, select = -c(user_name, raw_timestamp_part_1, raw_timestamp_part_2,\r\n                                        cvtd_timestamp, new_window, num_window, X))</code></pre>\r\n</div>\r\n<div id=\"handling-missing-values\" class=\"section level3\">\r\n<h3>Handling missing values</h3>\r\n<p>Closer look to the data showed that it contains some inappropriate symbols and true missing values, that is need to be handled of using replacement and imputing techniques.</p>\r\n<pre class=\"r\"><code>modelTrain[modelTrain == &quot;&quot;] &lt;- NA\r\n#looks like data comes from the Excel or similar editor and contains such a nice \r\nmodelTrain[modelTrain == &quot;#DIV/0!&quot;] &lt;- NA \r\n\r\n# Lookup for NA columns \r\nnaVars &lt;- data.frame(apply(modelTrain, 2, function(x) {sum(is.na(x))}))\r\nnaVars$toRemove &lt;- apply(naVars, 1, function(y) {y == length(modelTrain[, 1])})\r\ncolumnsToExclude &lt;- rownames(naVars[naVars$toRemove == &quot;TRUE&quot;, ])\r\ncolumnsToExclude</code></pre>\r\n<pre><code>## [1] &quot;kurtosis_yaw_belt&quot;     &quot;skewness_yaw_belt&quot;     &quot;kurtosis_yaw_dumbbell&quot;\r\n## [4] &quot;skewness_yaw_dumbbell&quot; &quot;kurtosis_yaw_forearm&quot;  &quot;skewness_yaw_forearm&quot;</code></pre>\r\n<pre class=\"r\"><code># Remove empty predictors\r\nmodelTrain &lt;- subset(modelTrain, select = -c(kurtosis_yaw_belt, skewness_yaw_belt, \r\n                                             kurtosis_yaw_dumbbell, skewness_yaw_dumbbell, \r\n                                             kurtosis_yaw_forearm, skewness_yaw_forearm ))</code></pre>\r\n</div>\r\n<div id=\"make-up-column-classes\" class=\"section level3\">\r\n<h3>Make up column classes</h3>\r\n<p>Before any modeling it is a good idea to check the variables classes and make sure that thea are all have the correct data type.</p>\r\n<pre class=\"r\"><code># Convert character columns into numeric\r\nlibrary(taRifx)\r\nmodelTrain &lt;- japply(modelTrain, which(sapply(modelTrain, class)==&quot;character&quot;),as.integer)\r\nmodelTrain &lt;- japply(modelTrain, which(sapply(modelTrain, class)==&quot;integer&quot;),as.integer)\r\nmodelTrain &lt;- japply(modelTrain, which(sapply(modelTrain, class)==&quot;factor&quot;),as.integer)\r\n\r\n# Convert classe to factor\r\nmodelTrain$classe &lt;- as.factor(modelTrain$classe)</code></pre>\r\n</div>\r\n<div id=\"tricky-moment\" class=\"section level3\">\r\n<h3>Tricky moment</h3>\r\n<p>Current number of variables most likely will cause the very long executing process and it would be better to reduce some dimensions. For example, instead of using nearZeroVar function I decided to play with lucky “cut-off” around number of missing values in the “nothing to impute about” columns.</p>\r\n<pre class=\"r\"><code>library(e1071)\r\ndim(modelTrain)</code></pre>\r\n<pre><code>## [1] 19622   147</code></pre>\r\n<pre class=\"r\"><code>modelTrain[is.na(modelTrain)] &lt;- 0\r\ncolAn &lt;- data.frame(colSums(subset(modelTrain, select = -classe) == 0))\r\nmodelTrain &lt;- modelTrain[, - which(colAn[, 1] &gt; 19000)] \r\ndim(modelTrain)</code></pre>\r\n<pre><code>## [1] 19622    53</code></pre>\r\n<p>Now we have ready to modeling clean and nice data set with a manageable number of predictors.</p>\r\n</div>\r\n</div>\r\n<div id=\"building-models\" class=\"section level2\">\r\n<h2>Building models</h2>\r\n<p>Before going deep into modeling better previously to take care of possible ways of improving productivity of your computer and use parallel processing if it is possible.</p>\r\n<pre class=\"r\"><code># Do parallel \r\nlibrary(doParallel)\r\nregisterDoParallel(cores=4)</code></pre>\r\n<div id=\"subsetting-data-for-cross-validation\" class=\"section level3\">\r\n<h3>Subsetting data for cross-validation</h3>\r\n<p>The idea of cross-validation is very nice and undoubtedly useful - to split data into two separated parts (around 70/30), learn the model at the biggest one and than estimate the out of sample error on the smallest one. The idea might be complicated with number of splits and their usage, but in case of our pretty huge data set with the managable amount of predictors I will follow of idea of just two random splits.</p>\r\n<pre class=\"r\"><code>library(caret)</code></pre>\r\n## Loading required package: ggplot2</code></pre>\r\n<pre class=\"r\"><code>inTrain &lt;- createDataPartition(y = modelTrain$classe, p = 0.7, list = FALSE)\r\ntraining &lt;- modelTrain[inTrain, ] \r\ntesting &lt;- modelTrain[-inTrain, ]\r\ndim(training); dim(testing)</code></pre>\r\n<pre><code>## [1] 13737    53</code></pre>\r\n<pre><code>## [1] 5885   53</code></pre>\r\n</div>\r\n<div id=\"modeling\" class=\"section level3\">\r\n<h3>Modeling</h3>\r\n<p>For classification task with more than two types of outcomes convinient to use classification trees algorithms and two different tree-based models will be trained - regular classification tree provided by “rpart”-method and random forest.</p>\r\n<pre class=\"r\"><code>library(caret)\r\nlibrary(randomForest)</code></pre>\r\n<pre><code>## randomForest 4.6-10\r\n## Type rfNews() to see new features/changes/bug fixes.</code></pre>\r\n<pre class=\"r\"><code>library(rattle)</code></pre>\r\n<pre class=\"r\"><code>modelRPART &lt;- train(classe ~ ., method = &quot;rpart&quot;, data = training,  na.action = na.pass)</code></pre>\r\n<pre class=\"r\"><code>print(modelRPART$finalModel)</code></pre>\r\n<pre><code>## n= 13737 \r\n## \r\n## node), split, n, loss, yval, (yprob)\r\n##       * denotes terminal node\r\n## \r\n##  1) root 13737 9831 1 (0.28 0.19 0.17 0.16 0.18)  \r\n##    2) roll_belt&lt; 130.5 12600 8706 1 (0.31 0.21 0.19 0.18 0.11)  \r\n##      4) pitch_forearm&lt; -33.35 1101   10 1 (0.99 0.0091 0 0 0) *\r\n##      5) pitch_forearm&gt;=-33.35 11499 8696 1 (0.24 0.23 0.21 0.2 0.12)  \r\n##       10) magnet_dumbbell_y&lt; 439.5 9718 6974 1 (0.28 0.18 0.24 0.19 0.11)  \r\n##         20) roll_forearm&lt; 123.5 6043 3582 1 (0.41 0.18 0.18 0.17 0.062) *\r\n##         21) roll_forearm&gt;=123.5 3675 2457 3 (0.077 0.18 0.33 0.23 0.19) *\r\n##       11) magnet_dumbbell_y&gt;=439.5 1781  880 2 (0.033 0.51 0.048 0.22 0.19) *\r\n##    3) roll_belt&gt;=130.5 1137   12 5 (0.011 0 0 0 0.99) *</code></pre>\r\n<pre class=\"r\"><code>predRPART &lt;- predict(modelRPART, subset(testing, select = -classe))\r\nconfusionMatrix(predRPART, testing$classe)</code></pre>\r\n\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    1    2    3    4    5\r\n##          1 1533  478  495  437  147\r\n##          2   22  385   22  174  145\r\n##          3  117  276  509  353  284\r\n##          4    0    0    0    0    0\r\n##          5    2    0    0    0  506\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.4984          \r\n##                  95% CI : (0.4855, 0.5112)\r\n##     No Information Rate : 0.2845          \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.3439          \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: 1 Class: 2 Class: 3 Class: 4 Class: 5\r\n## Sensitivity            0.9158  0.33802  0.49610   0.0000  0.46765\r\n## Specificity            0.6303  0.92351  0.78802   1.0000  0.99958\r\n## Pos Pred Value         0.4961  0.51471  0.33073      NaN  0.99606\r\n## Neg Pred Value         0.9496  0.85322  0.88104   0.8362  0.89288\r\n## Prevalence             0.2845  0.19354  0.17434   0.1638  0.18386\r\n## Detection Rate         0.2605  0.06542  0.08649   0.0000  0.08598\r\n## Detection Prevalence   0.5251  0.12710  0.26151   0.0000  0.08632\r\n## Balanced Accuracy      0.7730  0.63077  0.64206   0.5000  0.73362</code></pre>\r\n\r\n<pre class=\"r\"><code>fancyRpartPlot(modelRPART$finalModel) </code></pre>\r\n<p><img src=\"https://raw.githubusercontent.com/wfslithtaivs/wfslithtaivs.github.io/master/PML-Project_files/figure-html/unnamed-chunk-8-1.png\" title alt width=\"672\" /></p>\r\n\r\n<p>It is sad, but my computer failed to build random forest inside R Markdown report and I can't provide all the metrics and graphics that I planned to (this is the lesson to me to save all the versions (or just screenshot), not to live in hope of reproducibility of any code at any given moment). But believe me, the \"rf\" function gives 99% accuracy due to cross-validation on the separated test set and predicted all the test cases correctly.  \r\n\r\nUpdate: I got it at the last moment - build separately and manually add data to this report. Sorry for some mess. \r\n</p>\r\n<pre class=\"r\"><code>#modelRF &lt;- train(classe ~ ., method = &quot;rf&quot;, data = training)</code></pre>\r\n<pre class=\"r\"><code>print(modelRF$finalModel)</code></pre>\r\n<pre><code>\r\n## Call:\r\n## randomForest(x = x, y = y, mtry = param$mtry) \r\n##               Type of random forest: classification \r\n##                    Number of trees: 500\r\n## No. of variables tried at each split: 2\r\n##        OOB estimate of  error rate: 0.63%\r\n## Confusion matrix:\r\n##    1    2    3    4    5  class.error\r\n##   1 3904    1    1    0    0 0.0005120328\r\n##   2   14 2637    7    0    0 0.0079006772\r\n##   3    0   16 2374    6    0 0.0091819699\r\n##   4    0    0   32 2219    1 0.0146536412\r\n##   5    0    0    1    7 2517 0.0031683168\r\n</code></pre>\r\n<pre class=\"r\">predRF &lt;- prediction(modelRF, subset(testing, select = -classe))</code></pre>\r\n<pre class=\"r\">confusionMatrix(predRF, testing$classe)</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    1    2    3    4    5\r\n##          1 1672    9    0    0    0\r\n##          2    2 1129    9    0    0\r\n##         3    0    1 1015   12    2\r\n##         4    0    0    2  951    1\r\n##          5    0    0    0    1 1079\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9934         \r\n##                  95% CI : (0.991, 0.9953)\r\n##     No Information Rate : 0.2845           \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9916         \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: 1 Class: 2 Class: 3 Class: 4 Class: 5\r\n## Sensitivity             0.9988   0.9912   0.9893   0.9865   0.9972\r\n## Specificity           0.9979   0.9977   0.9969   0.9994   0.9998\r\n## Pos Pred Value          0.9946   0.9904   0.9854   0.9969   0.9991\r\n## Neg Pred Value         0.9995   0.9979   0.9977   0.9974   0.9994\r\n## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839\r\n## Detection Rate         0.2841   0.1918   0.1725   0.1616   0.1833\r\n## Detection Prevalence   0.2856   0.1937   0.1750   0.1621   0.1835\r\n## Balanced Accuracy     0.9983   0.9945   0.9931   0.9930   0.9985\r\n</code></pre>\r\n<pre class=\"r\"><code>varImpPlot(modelRF$finalModel) </code></pre>\r\n<p><img src=\"https://github.com/wfslithtaivs/wfslithtaivs.github.io/blob/master/PML-Project_files/figure-html/Rplot.png?raw=true\" title alt width=\"672\" /></p>\r\n<pre class=\"r\"><code>plot(modelRF$finalModel, log = \"y\")</code></pre>\r\nSee how the accuracy increase with the number of trees.\r\n<p><img src=\"https://github.com/wfslithtaivs/wfslithtaivs.github.io/blob/master/PML-Project_files/figure-html/Rplot01.png?raw=true\" title alt width=\"672\" /></p>\r\n<p>Random forest gives an excellent result and should be used for futher predictions.</p>\r\n</div>\r\n</div>\r\n<div id=\"test-case-prediction\" class=\"section level2\">\r\n<h2>Test case prediction</h2>\r\n<p>To use trained model on the test case all the data manipulations that were done with the train set should be applied to the test one.</p>\r\n<li>Code not shown, but it is the same as test set manipulations - cleansing and reducing.</i>\r\n<p>Final prediction of test cases gives the following vector:</p>\r\n<pre class=\"r\"><code>predTestTry &lt;- predict(modelRF, testTry)</code></pre>\r\n<p>Which is: B A B A A E D B A A B C B A E E A B B B, and showed the 100% accuracy in test cases prediction.</p>\r\n</div>","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}